{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled1.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/mayank2498/Skip-Gram-model-using-numpy/blob/master/word2vec_skip_gram.ipynb)"
      ]
    },
    {
      "metadata": {
        "id": "NjsuLTbvugQ6",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import string\n",
        "from nltk.corpus import stopwords \n",
        "\n",
        "def softmax(x):\n",
        "    \"\"\"Compute softmax values for each sets of scores in x.\"\"\"\n",
        "    e_x = np.exp(x - np.max(x))\n",
        "    return e_x / e_x.sum()\n",
        "\n",
        "class word2vec(object):\n",
        "    def __init__(self):\n",
        "        self.N = 10\n",
        "        self.X_train = []\n",
        "        self.y_train = []\n",
        "        self.window_size = 2\n",
        "        self.alpha = 0.001\n",
        "        self.words = []\n",
        "        self.word_index = {}\n",
        "\n",
        "    def initialize(self,V,data):\n",
        "        self.V = V\n",
        "        self.W = np.random.uniform(-0.8, 0.8, (self.V, self.N))\n",
        "        self.W1 = np.random.uniform(-0.8, 0.8, (self.N, self.V))\n",
        "        self.words = data\n",
        "        for i in range(len(data)):\n",
        "            self.word_index[data[i]] = i\n",
        "#        self.W = np.random.randn(self.V,self.N)\n",
        "#        self.W1 = np.random.randn(self.N,self.V)\n",
        "    \n",
        "    def feed_forward(self,X):\n",
        "        self.h = np.dot(self.W.T,X).reshape(self.N,1)\n",
        "        self.u = np.dot(self.W1.T,self.h)\n",
        "        #print(self.u)\n",
        "        self.y = softmax(self.u)  \n",
        "        return self.y\n",
        "        \n",
        "    def backpropagate(self,x,t):\n",
        "        e = self.y - np.asarray(t).reshape(self.V,1)\n",
        "        # e.shape is V x 1\n",
        "      \n",
        "        dLdW1 = np.dot(self.h,e.T)\n",
        "        X = np.array(x).reshape(self.V,1)\n",
        "        dLdW = np.dot(X, np.dot(self.W1,e).T)\n",
        "        self.W1 = self.W1 - self.alpha*dLdW1\n",
        "        self.W = self.W - self.alpha*dLdW\n",
        "        \n",
        "    def train(self,epochs):\n",
        "        for x in range(1,epochs):  \n",
        "              \n",
        "            self.loss = 0\n",
        "            for j in range(len(self.X_train)):\n",
        "                self.feed_forward(self.X_train[j])\n",
        "                self.backpropagate(self.X_train[j],self.y_train[j])\n",
        "                C = 0\n",
        "                for m in range(self.V):\n",
        "                    if(self.y_train[j][m]):\n",
        "                        self.loss += -1*self.u[m][0]\n",
        "                        C += 1\n",
        "                self.loss += C*np.log(np.sum(np.exp(self.u)))\n",
        "            print(\"epoch \",x, \" loss = \",self.loss)\n",
        "            self.alpha *= 1/( (1+self.alpha*x) )\n",
        "    def predict(self,word,number_of_predictions):\n",
        "        if word in self.words:\n",
        "            index = self.word_index[word]\n",
        "            X = [0 for i in range(self.V)]\n",
        "            X[index] = 1\n",
        "            prediction = self.feed_forward(X)\n",
        "            output = {}\n",
        "            for i in range(self.V):\n",
        "                output[prediction[i][0]] = i\n",
        "            \n",
        "            top_context_words = []\n",
        "            for k in sorted(output,reverse=True):\n",
        "                top_context_words.append(self.words[output[k]])\n",
        "                if(len(top_context_words)>=number_of_predictions):\n",
        "                    break\n",
        "    \n",
        "            return top_context_words\n",
        "        else:\n",
        "            print(\"Word not found in dicitonary\")\n",
        "\n",
        "\n",
        "def preprocessing(corpus):\n",
        "    stop_words = set(stopwords.words('english'))    \n",
        "    training_data = []\n",
        "    sentences = corpus.split(\".\")\n",
        "    for i in range(len(sentences)):\n",
        "        sentences[i] = sentences[i].strip()\n",
        "        sentence = sentences[i].split()\n",
        "        x = [word.strip(string.punctuation) for word in sentence if word not in stop_words]\n",
        "        x = [word.lower() for word in x]\n",
        "        training_data.append(x)\n",
        "    return training_data\n",
        "    \n",
        "\n",
        "def prepare_data_for_training(sentences,w2v):\n",
        "    data = {}\n",
        "    for sentence in sentences:\n",
        "        for word in sentence:\n",
        "            if word not in data:\n",
        "                data[word] = 1\n",
        "            else:\n",
        "                data[word] += 1\n",
        "    V = len(data)\n",
        "    data = sorted(list(data.keys()))\n",
        "    vocab = {}\n",
        "    for i in range(len(data)):\n",
        "        vocab[data[i]] = i\n",
        "    \n",
        "    #for i in range(len(words)):\n",
        "    for sentence in sentences:\n",
        "        for i in range(len(sentence)):\n",
        "            center_word = [0 for x in range(V)]\n",
        "            center_word[vocab[sentence[i]]] = 1\n",
        "            context = [0 for x in range(V)]\n",
        "            for j in range(i-w2v.window_size,i+w2v.window_size):\n",
        "                if i!=j and j>=0 and j<len(sentence):\n",
        "                    context[vocab[sentence[j]]] += 1\n",
        "            w2v.X_train.append(center_word)\n",
        "            w2v.y_train.append(context)\n",
        "    w2v.initialize(V,data)\n",
        "\n",
        "    return w2v.X_train,w2v.y_train    \n",
        "\n",
        "corpus = \"\"\n",
        "corpus += \"The World Wide Web (WWW), also called the Web, is an information space where documents and other web resources are identified by Uniform Resource Locators (URLs), interlinked by hypertext links, and accessible via the Internet.[1] English scientist Tim Berners-Lee invented the World Wide Web in 1989. He wrote the first web browser in 1990 while employed at CERN in Switzerland.[2][3] The browser was released outside CERN in 1991, first to other research institutions starting in January 1991 and to the general public on the Internet in August 1991. The World Wide Web has been central to the development of the Information Age and is the primary tool billions of people use to interact on the Internet.[4][5][6] Web pages are primarily text documents formatted and annotated with Hypertext Markup Language (HTML).[7] In addition to formatted text, web pages may contain images, video, audio, and software components that are rendered in the user's web browser as coherent pages of multimedia content. Embedded hyperlinks permit users to navigate between web pages. Multiple web pages with a common theme, a common domain name, or both, make up a website. Website content can largely be provided by the publisher, or interactively where users contribute content or the content depends upon the users or their actions. Websites may be mostly informative, primarily for entertainment, or largely for commercial, governmental, or non-governmental organisational purpose \"\n",
        "\n",
        "training_data = preprocessing(corpus)\n",
        "w2v = word2vec()\n",
        "X,y = prepare_data_for_training(training_data,w2v)\n",
        "w2v.train(3000)    \n",
        "\n",
        "    "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "jMOwdbi-rAJr",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "w2v.predict(\"www\",5)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "URsIITgU31X1",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "with open('singles.txt', 'r') as myfile:\n",
        "    data=myfile.read().replace('\\n', '')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "2vfCQsy94njz",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "data"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "pqqqGi794_bY",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "training_data = preprocessing(data)\n",
        "w2v = word2vec()\n",
        "X,y = prepare_data_for_training(training_data,w2v)\n",
        "w2v.train(1000)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "jXF1dT7L5MmX",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "f4bc7776-660b-457e-aaee-a0af68aa43b2"
      },
      "cell_type": "code",
      "source": [
        "w2v.predict(\"sense\",5)"
      ],
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['movies', 'humour', 'going', 'times', 'supporting']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 56
        }
      ]
    }
  ]
}